{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-13T05:45:44.888977Z",
     "start_time": "2025-10-13T05:45:44.884463Z"
    }
   },
   "source": "paragraph = \"\"\"ChemAnalyst is a digital platform, which keeps a real-time eye on the chemicals and petrochemicals market fluctuations, thus, enabling its customers to make wise business decisions. With over 450 chemical products traded globally, we bring detailed market information and pricing data at your fingertip's. Our real-time pricing and commentary updates enable users to stay acquainted with new commercial opportunities. \"\"\"",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenization",
   "id": "c9eb70dade4fe290"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T05:45:45.589855Z",
     "start_time": "2025-10-13T05:45:45.582829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(paragraph, language='english')\n",
    "print(sentences)"
   ],
   "id": "cccf2b21e32dc925",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ChemAnalyst is a digital platform, which keeps a real-time eye on the chemicals and petrochemicals market fluctuations, thus, enabling its customers to make wise business decisions.', \"With over 450 chemical products traded globally, we bring detailed market information and pricing data at your fingertip's.\", 'Our real-time pricing and commentary updates enable users to stay acquainted with new commercial opportunities.']\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T05:45:46.459777Z",
     "start_time": "2025-10-13T05:45:46.445632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words = word_tokenize(paragraph)\n",
    "print(words)"
   ],
   "id": "b221e784a5e25306",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ChemAnalyst', 'is', 'a', 'digital', 'platform', ',', 'which', 'keeps', 'a', 'real-time', 'eye', 'on', 'the', 'chemicals', 'and', 'petrochemicals', 'market', 'fluctuations', ',', 'thus', ',', 'enabling', 'its', 'customers', 'to', 'make', 'wise', 'business', 'decisions', '.', 'With', 'over', '450', 'chemical', 'products', 'traded', 'globally', ',', 'we', 'bring', 'detailed', 'market', 'information', 'and', 'pricing', 'data', 'at', 'your', 'fingertip', \"'s\", '.', 'Our', 'real-time', 'pricing', 'and', 'commentary', 'updates', 'enable', 'users', 'to', 'stay', 'acquainted', 'with', 'new', 'commercial', 'opportunities', '.']\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T05:45:47.934080Z",
     "start_time": "2025-10-13T05:45:47.928073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "punc_words = wordpunct_tokenize(paragraph)\n",
    "print(punc_words)"
   ],
   "id": "242480f80c799076",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ChemAnalyst', 'is', 'a', 'digital', 'platform', ',', 'which', 'keeps', 'a', 'real', '-', 'time', 'eye', 'on', 'the', 'chemicals', 'and', 'petrochemicals', 'market', 'fluctuations', ',', 'thus', ',', 'enabling', 'its', 'customers', 'to', 'make', 'wise', 'business', 'decisions', '.', 'With', 'over', '450', 'chemical', 'products', 'traded', 'globally', ',', 'we', 'bring', 'detailed', 'market', 'information', 'and', 'pricing', 'data', 'at', 'your', 'fingertip', \"'\", 's', '.', 'Our', 'real', '-', 'time', 'pricing', 'and', 'commentary', 'updates', 'enable', 'users', 'to', 'stay', 'acquainted', 'with', 'new', 'commercial', 'opportunities', '.']\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T05:46:58.891600Z",
     "start_time": "2025-10-13T05:46:58.870602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(paragraph)\n",
    "tokens"
   ],
   "id": "5bde2e86d24aed4d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ChemAnalyst',\n",
       " 'is',\n",
       " 'a',\n",
       " 'digital',\n",
       " 'platform',\n",
       " ',',\n",
       " 'which',\n",
       " 'keeps',\n",
       " 'a',\n",
       " 'real-time',\n",
       " 'eye',\n",
       " 'on',\n",
       " 'the',\n",
       " 'chemicals',\n",
       " 'and',\n",
       " 'petrochemicals',\n",
       " 'market',\n",
       " 'fluctuations',\n",
       " ',',\n",
       " 'thus',\n",
       " ',',\n",
       " 'enabling',\n",
       " 'its',\n",
       " 'customers',\n",
       " 'to',\n",
       " 'make',\n",
       " 'wise',\n",
       " 'business',\n",
       " 'decisions.',\n",
       " 'With',\n",
       " 'over',\n",
       " '450',\n",
       " 'chemical',\n",
       " 'products',\n",
       " 'traded',\n",
       " 'globally',\n",
       " ',',\n",
       " 'we',\n",
       " 'bring',\n",
       " 'detailed',\n",
       " 'market',\n",
       " 'information',\n",
       " 'and',\n",
       " 'pricing',\n",
       " 'data',\n",
       " 'at',\n",
       " 'your',\n",
       " \"fingertip's.\",\n",
       " 'Our',\n",
       " 'real-time',\n",
       " 'pricing',\n",
       " 'and',\n",
       " 'commentary',\n",
       " 'updates',\n",
       " 'enable',\n",
       " 'users',\n",
       " 'to',\n",
       " 'stay',\n",
       " 'acquainted',\n",
       " 'with',\n",
       " 'new',\n",
       " 'commercial',\n",
       " 'opportunities',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Stemming",
   "id": "146caf127611f83a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:34:20.376692Z",
     "start_time": "2025-10-13T08:34:20.372691Z"
    }
   },
   "cell_type": "code",
   "source": "from nltk.stem import PorterStemmer",
   "id": "bb7357802b9e7ebe",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:34:20.689232Z",
     "start_time": "2025-10-13T08:34:20.685232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = [\"go\", \"gone\", \"going\", \"goes\", \"eats\", \"eating\", \"eat\", \"work\", \"working\", \"worked\"]\n",
    "ps = PorterStemmer()"
   ],
   "id": "2327a97510001c89",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:34:21.224417Z",
     "start_time": "2025-10-13T08:34:21.205415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for word in words:\n",
    "    print(word, \"-->\", ps.stem(word))"
   ],
   "id": "18db1f8cc1953bde",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go --> go\n",
      "gone --> gone\n",
      "going --> go\n",
      "goes --> goe\n",
      "eats --> eat\n",
      "eating --> eat\n",
      "eat --> eat\n",
      "work --> work\n",
      "working --> work\n",
      "worked --> work\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:34:21.462746Z",
     "start_time": "2025-10-13T08:34:21.445748Z"
    }
   },
   "cell_type": "code",
   "source": "from nltk.stem import RegexpStemmer",
   "id": "441303d511ca5a8b",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:34:21.696118Z",
     "start_time": "2025-10-13T08:34:21.677119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reg = RegexpStemmer('ing$|s$|e$|able$|ed$|es$', min=4)\n",
    "\n",
    "for word in words:\n",
    "    print(word, \"-->\", reg.stem(word))"
   ],
   "id": "d1003f2232a6b915",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go --> go\n",
      "gone --> gon\n",
      "going --> go\n",
      "goes --> go\n",
      "eats --> eat\n",
      "eating --> eat\n",
      "eat --> eat\n",
      "work --> work\n",
      "working --> work\n",
      "worked --> work\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:34:21.882847Z",
     "start_time": "2025-10-13T08:34:21.878846Z"
    }
   },
   "cell_type": "code",
   "source": "from nltk.stem import SnowballStemmer",
   "id": "b7a717a23e8cf9de",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:34:22.133995Z",
     "start_time": "2025-10-13T08:34:22.121991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "snow = SnowballStemmer('english')\n",
    "for word in words:\n",
    "    print(word, \"-->\", snow.stem(word))"
   ],
   "id": "72d2cf4e06eccbdb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go --> go\n",
      "gone --> gone\n",
      "going --> go\n",
      "goes --> goe\n",
      "eats --> eat\n",
      "eating --> eat\n",
      "eat --> eat\n",
      "work --> work\n",
      "working --> work\n",
      "worked --> work\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Lemmatization",
   "id": "2f4c0d74690e1bf4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:34:23.249106Z",
     "start_time": "2025-10-13T08:34:23.230684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ],
   "id": "1e644872e4a261f0",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:31:53.953383Z",
     "start_time": "2025-10-13T08:31:53.906381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')"
   ],
   "id": "b13e352adad3f8a1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\D S\n",
      "[nltk_data]     Patwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:34:26.468298Z",
     "start_time": "2025-10-13T08:34:26.463295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "POS -\n",
    "Noun-n\n",
    "Verb-v\n",
    "Adjective-a\n",
    "Abverb-r\n",
    "'''\n",
    "\n",
    "lemmatizer.lemmatize('going', pos='v')"
   ],
   "id": "aee3c8c97dc48843",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:34:54.938904Z",
     "start_time": "2025-10-13T08:34:54.927348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for word in words:\n",
    "    print(word, \"-->\", lemmatizer.lemmatize(word, pos='v'))"
   ],
   "id": "3c3384a50f1a7496",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go --> go\n",
      "gone --> go\n",
      "going --> go\n",
      "goes --> go\n",
      "eats --> eat\n",
      "eating --> eat\n",
      "eat --> eat\n",
      "work --> work\n",
      "working --> work\n",
      "worked --> work\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6be30eba0c5c17d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "806df25c936470e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Stopword removal",
   "id": "42086eab52b15e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:47:21.697333Z",
     "start_time": "2025-10-13T08:47:21.690338Z"
    }
   },
   "cell_type": "code",
   "source": "paragraph = \"\"\"In machine learning, a Type I error (false positive) occurs when a model incorrectly predicts a positive outcome, rejecting the null hypothesis when it is actually true. A Type II error (false negative) happens when a model fails to detect a positive outcome, failing to reject a false null hypothesis. These errors are inversely related; reducing the chance of one increases the chance of the other\"\"\"",
   "id": "801b0c549a67ca60",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:39:17.326316Z",
     "start_time": "2025-10-13T08:39:17.251234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ],
   "id": "6b755cacf192baf6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\D S\n",
      "[nltk_data]     Patwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:40:28.346532Z",
     "start_time": "2025-10-13T08:40:28.327342Z"
    }
   },
   "cell_type": "code",
   "source": "stopwords.words('english')",
   "id": "3510e011777d08b1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:41:24.339526Z",
     "start_time": "2025-10-13T08:41:24.329491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stemmer = PorterStemmer()\n",
    "sentences = nltk.sent_tokenize(paragraph)"
   ],
   "id": "e524e8e119183b68",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:43:43.160429Z",
     "start_time": "2025-10-13T08:43:43.131296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# apply stopwords and filter and then apply stemming\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in stopwords.words('english')]\n",
    "    sentences[i] = ' '.join(words)\n"
   ],
   "id": "b34f3b48cf74a54b",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:43:48.976782Z",
     "start_time": "2025-10-13T08:43:48.962784Z"
    }
   },
   "cell_type": "code",
   "source": "sentences",
   "id": "9e5438e48b6e0ec5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in machin learn , type i error ( fals posit ) occur model incorrectli predict posit outcom , reject null hypothesi actual true .',\n",
       " 'a type ii error ( fals neg ) happen model fail detect posit outcom , fail reject fals null hypothesi .',\n",
       " 'these error invers relat ; reduc chanc one increas chanc']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:47:28.269105Z",
     "start_time": "2025-10-13T08:47:28.239589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# apply stopwords and filter and then apply snowball stemming\n",
    "from nltk.stem import SnowballStemmer\n",
    "snow = SnowballStemmer('english')\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [snow.stem(word) for word in words if word not in stopwords.words('english')]\n",
    "    sentences[i] = ' '.join(words)\n",
    "\n",
    "sentences\n"
   ],
   "id": "c51505f036be0cbb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machin learn , type error ( fal posit ) occur model incorrect predict posit outcom , reject null hypothesi actual true .',\n",
       " 'type ii error ( fal neg ) happen model fail detect posit outcom , fail reject fal null hypothesi .',\n",
       " 'error inver relat ; reduc chanc one increa chanc']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:49:35.517968Z",
     "start_time": "2025-10-13T08:49:35.492968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# apply stopwords and filter and then apply lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word, pos='v') for word in words if word not in stopwords.words('english')]\n",
    "    sentences[i] = ' '.join(words)\n",
    "\n",
    "sentences\n"
   ],
   "id": "683b2454f4d2fd19",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machin learn , type error ( fal posit ) occur model incorrect predict posit outcom , reject null hypothesi actual true .',\n",
       " 'type ii error ( fal neg ) happen model fail detect posit outcom , fail reject fal null hypothesi .',\n",
       " 'error inver relat ; reduc chanc one increa chanc']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "69cbbc12d2e9f4f0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
